\documentclass{article}[9pt]
% \usepackage[utf8]{inputenc}%
% \usepackage{tikz}
% \usepackage{cfr-lm}%
\usepackage{jmlr2e}
\usepackage[T1]{fontenc}%
% \usepackage{physics}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage[margin=2cm]{geometry}
% \usepackage{changepage}
\usepackage{fontspec}
\usepackage{minted}
\usepackage{tcolorbox}
\usepackage{lmodern}
\usepackage{xcolor}
\usepackage{nccmath}
\usepackage{lettrine}
\usepackage{multicol}
% \usepackage{fontawesome}
\usemintedstyle{bw}
\usepackage{hyperref}
\hypersetup{
    colorlinks=false, pdfborder={0 0 0}
}


\newcommand\blfootnote[1]{%
  \begingroup
  \renewcommand\thefootnote{}\footnote{#1}%
  \addtocounter{footnote}{-1}%
  \endgroup
}


\newtcbox{\codebox}[1][black]{on line, arc=2pt,colback=#1!10!white,colframe=white, before upper={\rule[-3pt]{0pt}{10pt}},boxrule=1pt, boxsep=0pt,left=2pt,right=2pt,top=1pt,bottom=.5pt}
% \definecolor{antwhite}{HTML}{323333}
\newcommand{\code}[3][]{\codebox{\mintinline[#1]{#2}{#3}}}

\title{Tittel}%
\author{Brage Wiseth}%
\date{\today}%

% \setmainfont{FreeSans}
% \setmainfont{SF Pro Display}
% \setmainfont{IBM Plex Sans}
% \setmainfont{TeX Gyre Heros}
% \setmainfont{Inter}
% \setmainfont{Iosevka Quasi}

\setmonofont[Scale=MatchLowercase]{DM Mono}
% \setmonofont[medium]{Jetbrains Mono}

\begin{document}%
% \maketitle%
% \normalfont
    \begin{center}
        \textbf{\LARGE Image Classification with Fine-tuning of Pre-trained Models}
    \end{center}
    \vspace{0.5em}
    \begin{center}
        \textbf{Brage Wiseth}
    \end{center}
    \begin{center}
        \textbf{\today}
    \end{center}
    \vspace{0.5em}


% \begin{abstract}%   <- trailing '%' for backward compatibility of .sty file
% \lettrine{T}{}his paper describes the mixtures-of-trees model, a probabilistic 
% model for discrete multidimensional domains.  Mixtures-of-trees 
% generalize the probabilistic trees of 
% in a different and complementary direction to that of Bayesian networks.
% We present efficient algorithms for learning mixtures-of-trees 
% models in maximum likelihood and Bayesian frameworks. 
% We also discuss additional efficiencies that can be
% obtained when data are ``sparse,'' and we present data 
% structures and algorithms that exploit such sparseness.
% Experimental results demonstrate the performance of the 
% model for both density estimation and classification. 
% We also discuss the sense in which tree-based classifiers
% perform an implicit form of feature selection, and demonstrate
% a resulting insensitivity to irrelevant attributes.
% \end{abstract}

\begin{figure*}[!ht]
    \centering
    \includegraphics[width=0.8\textwidth]{training.png}
    \caption{Training of the model. The training loss is shown in blue, and the validation 
    loss is shown in orange. The model was trained for 100 epochs, but we can see that the model
    starts to overfit moments after the first epoch. Indicating that the pre-trained model is
    already very good at classifying the images.}
    \label{fig:training}
\end{figure*}

% \begin{multicols}{2}
The model used was a pre-trained ResNet18 model, which was fine-tuned on the ImageNet dataset
using the PyTorch library. The model was fine-tuned with a modified linear head, which was
replaced with a new linear layer with 6 output units. The model was trained using the cross-entropy
loss function. The optimal learning rate and optimizer of those
used were found to be the Adam optimizer with a learning rate of 0.001 with a weight decay of 0.0001.
The model was trained for
100 epochs. The final test accuracy was 91\% and the mean average precision (mAP) was 92\%. 
% \end{multicols}

\clearpage

\begin{figure*}[!t]
    \centering
    \includegraphics[width=0.4\textwidth]{original-image.png}
    \caption{Original image}
    \label{fig:original-image}
\end{figure*}
\begin{figure*}[h]
    \centering
    \includegraphics[width=1.0\textwidth]{featuremaps.png}
    \caption{Feature maps of five activations of the model, from 
        shallow to deep layers top to bottom. The percentage of non-zero activations in the 
        feature maps across 200 images are
        [0.83 0.77  0.52  0.51  0.46 ] top to bottom. So the shallow layers have more non-zero activations
        than the deeper layers.}
    \label{fig:featuremaps}
\end{figure*}

\clearpage

\begin{figure*}[h]
    \centering
    \includegraphics[width=1.0\textwidth]{best_worst_first_class.png}
    \caption{Ten best and worst images of the buildings class}
    \label{fig:featuremaps}
\end{figure*}


\begin{figure*}[h]
    \centering
    \includegraphics[width=1.0\textwidth]{best_worst_second_class.png}
    \caption{Ten best and worst images of the forest class}
    \label{fig:featuremaps}
\end{figure*}


\begin{figure*}[h]
    \centering
    \includegraphics[width=1.0\textwidth]{best_worst_sixth_class.png}
    \caption{Ten best and worst images of the street class}
    \label{fig:featuremaps}
\end{figure*}

\end{document}
